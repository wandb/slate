
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>API Reference</title>
    <link href="images/favicon.png" rel="icon" type="image/png" />


    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .cd {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .nl {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[&quot;python--tensorflow&quot;,&quot;python--keras&quot;,&quot;python--pytorch&quot;]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo.png" class="logo" alt="Logo" />
        <div class="lang-selector">
              <a href="#" data-language-name="python--tensorflow">TensorFlow</a>
              <a href="#" data-language-name="python--keras">Keras</a>
              <a href="#" data-language-name="python--pytorch">Pytorch</a>
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc" class="toc-list-h1">
          <li>
            <a href="#introduction" class="toc-h1 toc-link" data-title="Introduction">Introduction</a>
          </li>
          <li>
            <a href="#getting-started" class="toc-h1 toc-link" data-title="Getting Started">Getting Started</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#automated-environments" class="toc-h2 toc-link" data-title="Automated Environments">Automated Environments</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#tracking-models" class="toc-h1 toc-link" data-title="Tracking Models">Tracking Models</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#configurations" class="toc-h2 toc-link" data-title="Configurations">Configurations</a>
                  </li>
                  <li>
                    <a href="#metrics" class="toc-h2 toc-link" data-title="Metrics">Metrics</a>
                  </li>
                  <li>
                    <a href="#saving-models" class="toc-h2 toc-link" data-title="Saving Models">Saving Models</a>
                  </li>
                  <li>
                    <a href="#saving-code-state" class="toc-h2 toc-link" data-title="Saving Code State">Saving Code State</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#hyperparamer-search" class="toc-h1 toc-link" data-title="Hyperparamer Search">Hyperparamer Search</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#running-a-parameter-sweep" class="toc-h2 toc-link" data-title="Running a Parameter Sweep">Running a Parameter Sweep</a>
                  </li>
                  <li>
                    <a href="#sweep-yaml-file" class="toc-h2 toc-link" data-title="Sweep.yaml File">Sweep.yaml File</a>
                  </li>
              </ul>
          </li>
      </div>
        <ul class="toc-footer">
            <li><a href='https://wandb.ai'>Login to wandb</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id='introduction'>Introduction</h1><pre class="highlight shell tab-shell"><code><span class="c"># Install wandb</span>
pip install wandb
</code></pre>
<p>Wandb is a tool for people building machine learning models.  </p>

<p>Wandb helps with:</p>

<ol>
<li> Tracking, saving and reproducing models.<br></li>
<li> Visualizing results across models.</li>
<li> Automating large-scale hyperparameter search.</li>
</ol>
<h1 id='getting-started'>Getting Started</h1><pre class="highlight shell tab-shell"><code><span class="c"># Initialize wandb in the root directory of your project</span>
wandb init
</code></pre>
<blockquote>
<p>Near the top of your code add initialization code:</p>
</blockquote>
<pre class="highlight python tab-python"><code><span class="c"># Inside my model training code</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c"># Run my complicated training loop...</span>
</code></pre>
<blockquote>
<p>Modify your command to run training</p>
</blockquote>
<pre class="highlight shell tab-shell"><code><span class="c"># The old way to run your training script (still works, but no magic)</span>
python learn.py
</code></pre><pre class="highlight shell tab-shell"><code><span class="c"># The new way to run your training script (saves logs, and so much more...)</span>
wandb run learn.py
</code></pre>
<p>It&#39;s easy to configure Wandb to work with one of your projects.  </p>

<p>Sign up for an account at <a href="https://wandb.ai">https://wandb.ai</a></p>

<p>You will be prompted for a team name and a project name.  This will create a
wandb directory that contains a settings file with your entity name and
project name.</p>

<p>Now every time you run your training script with wandb, a new record will
be added to https://wandb.ai/$ENTITY_NAME/$PROJECT_NAME.  Your training logs
will be saved along with a snapshot of your latest commit.</p>

<aside class="notice">
You can always rerun *wandb init* to change your project's settings.
</aside>
<h2 id='automated-environments'>Automated Environments</h2><pre class="highlight shell tab-shell"><code><span class="c"># This is secret and shouldn't be checked into version control</span>
<span class="nv">WANDB_API_KEY</span><span class="o">=</span><span class="nv">$YOUR_API_KEY</span>
<span class="c"># Description is optional</span>
<span class="nv">WANDB_DESCRIPTION</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SHORT_MESSAGE</span><span class="s2">"</span>
</code></pre><pre class="highlight shell tab-shell"><code><span class="c"># Only needed if you don't checkin the wandb/settings file</span>
<span class="nv">WANDB_ENTITY</span><span class="o">=</span><span class="nv">$username</span>
<span class="nv">WANDB_PROJECT</span><span class="o">=</span><span class="nv">$project</span>
</code></pre><pre class="highlight python tab-python"><code><span class="c"># Only needed if you don't checkin the wandb/settings file</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s">'wandb'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre><pre class="highlight python tab-python"><code><span class="c"># If you don't want to call your script with the wandb run wrapper</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WANDB_MODE'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'run'</span>
</code></pre>
<p>When you are running your script in an automated environment, you can set environment variables before the script runs or within the script.  <strong>WANDB_API_KEY</strong> must be set if <code>wandb login</code> hasn&#39;t been run on the remote machine.  Optionally you can set <strong>WANDB_DESCRIPTION</strong> to something that will become the name of your runs in the UI.</p>

<p>If you have run <code>wandb init</code> in the directory of your training script a directory named <em>wandb</em> will be created and will contain a settings file with your entity and project set in it.  If you check this file in, you do not need to set <strong>WANDB_ENTITY</strong> and <strong>WANDB_PROJECT</strong>.  If you want to set these dynamically or don&#39;t wish to checkin this file, you can use the environment variables.</p>

<p>If you chose not to checkin the <code>wandb/settings</code> file with your project, a directory named <code>wandb</code> must exist in the same directory that the script is run in.</p>

<p>If you don&#39;t want to run your script with the <code>wandb run</code> wrapper, you must set the <strong>WANDB_MODE</strong> environment variable to &quot;run&quot; to have your runs synced to the cloud.  Otherwise, just be sure your automated process calls your script like: <code>wandb run python train.py --args</code></p>
<h1 id='tracking-models'>Tracking Models</h1><h2 id='configurations'>Configurations</h2><pre class="highlight python tab-python--tensorflow"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">config</span> <span class="c"># get the config object</span>
<span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>
                    <span class="c"># with a run every time they're set</span>

<span class="n">flags</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s">'data_dir'</span><span class="p">,</span> <span class="s">'/tmp/data'</span><span class="p">)</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'batch_size'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'Batch size.'</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span><span class="p">)</span>  <span class="c"># adds all of the tensorflow flags as config variables</span>
</code></pre><pre class="highlight python tab-python--keras"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">config</span> <span class="c"># get the config object</span>
<span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>
                    <span class="c"># with a run every time they're set</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s">'N'</span><span class="p">,</span>
                     <span class="n">help</span><span class="o">=</span><span class="s">'input batch size for training (default: 8)'</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c"># adds all of the arguments as config variables</span>
</code></pre><pre class="highlight python tab-python--pytorch"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">config</span> <span class="c"># get the config object</span>
<span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>
                    <span class="c"># with a run every time they're set</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s">'N'</span><span class="p">,</span>
                     <span class="n">help</span><span class="o">=</span><span class="s">'input batch size for training (default: 8)'</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c"># adds all of the arguments as config variables</span>
</code></pre>
<p>Configurations is a way of automatically tracking the hyperparameters you used
to build your model.</p>

<p>You can set the configuration values directly and access them as ordinary variables, or
you can import variables from tensorflow flags or argparse objects to integrate
with pre-existing code.</p>
<h3 id='file-based-configs'>File-Based Configs</h3>
<blockquote>
<p>(Optional) Create a config-defaults file to automatically load hyperparameters
into the config variable.</p>
</blockquote>
<pre class="highlight yaml tab-yaml"><code><span class="c1"># sample config-defaults file</span>
<span class="na">epochs</span><span class="pi">:</span>
  <span class="na">desc</span><span class="pi">:</span> <span class="s">Number of epochs to train over</span>
  <span class="na">value</span><span class="pi">:</span> <span class="s">100</span>
<span class="na">batch_size</span><span class="pi">:</span>
  <span class="na">desc</span><span class="pi">:</span> <span class="s">Size of each mini-batch</span>
  <span class="na">value</span><span class="pi">:</span> <span class="s">32</span>
</code></pre>
<p>You can create a file called config-defaults.yaml and it will automatically
be loaded into the config variable.</p>

<p>You can tell wandb to load different config files with the command</p>

<blockquote>
<p>Automatically load the yaml file into the config object</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run train.py
</code></pre>
<blockquote>
<p>Change the config file used to load the config object</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run --configs special-configs.yaml
</code></pre>
<blockquote>
<p>Mutiple config files are allowed are allowed</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run --configs special-configs.yaml,extra-configs.yaml
</code></pre><h2 id='metrics'>Metrics</h2><pre class="highlight python tab-python--tensorflow"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">history</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">summary</span>

<span class="c"># Start training</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="c"># Run optimization op (backprop)</span>
      <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>
      <span class="c"># Calculate batch loss and accuracy</span>
      <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss_op</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span>
                                                               <span class="n">Y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>

      <span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span><span class="s">'acc'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">:</span><span class="n">loss</span><span class="p">})</span>   <span class="c"># log accuracy and loss</span>
      <span class="n">summary</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span>  
      <span class="n">summary</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

</code></pre><pre class="highlight python tab-python--keras"><code>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">history</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">summary</span>

<span class="k">def</span> <span class="nf">log_discriminator</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="p">):</span>
  <span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span>
            <span class="s">'loss'</span><span class="p">:</span> <span class="n">logs</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span>
            <span class="s">'acc'</span><span class="p">:</span> <span class="n">logs</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]})</span>
  <span class="n">summary</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>


<span class="n">wandb_logging_callback</span> <span class="o">=</span> <span class="n">LambdaCallback</span><span class="p">(</span><span class="n">on_epoch_end</span><span class="o">=</span><span class="n">log_generator</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">wandb_logging_callback</span><span class="p">])</span>
</code></pre><pre class="highlight python tab-python--pytorch"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">history</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">summary</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>

  <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'model'</span><span class="p">)</span>

  <span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span><span class="s">"Train Loss"</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span>
                   <span class="s">"Test Loss"</span><span class="p">:</span> <span class="n">test_loss</span><span class="p">})</span>
  <span class="n">summary</span><span class="p">[</span><span class="s">"Test Accuracy"</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_accuracy</span>

</code></pre>
<p>Metrics are a way of automatically tracking information about how the model
is performing.</p>
<h3 id='history'>History</h3>
<p>The history object is used to track metrics that change as the model runs.  Every
time the history object is updated the metrics are tracked.</p>
<h3 id='summary'>Summary</h3>
<p>The summary statistics are used to track single metrics per model.  If a summary
metric is modified, only the updated state is saved.</p>
<h3 id='keras-callback'>Keras Callback</h3>
<blockquote>
<p>Simpler way to track metrics in keras using WandbKerasCallback</p>
</blockquote>
<pre class="highlight python tab-python--keras"><code><span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">from</span> <span class="nn">wandb.wandb_keras</span> <span class="kn">import</span> <span class="n">WandbKerasCallback</span>

<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">config</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">WandbKerasCallback</span><span class="p">()])</span>
</code></pre>
<p>If you are using keras, you can use the WandbKerasCallback to automatically save
all the metrics and the loss values tracked in <code>model.fit</code>.</p>
<h2 id='saving-models'>Saving Models</h2><pre class="highlight python tab-python--keras"><code><span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">from</span> <span class="nn">wandb.wandb_keras</span> <span class="kn">import</span> <span class="n">WandbKerasCallback</span>

<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">WandbKerasCallback</span><span class="p">()])</span>

<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run</span><span class="o">.</span><span class="nb">dir</span><span class="p">,</span> <span class="s">"model.h5"</span><span class="p">))</span> <span class="c">#</span>
</code></pre>
<p>Wandb will save to the cloud any files put in wandb&#39;s run directory.</p>

<p>Wandb&#39;s run directories are inside the wandb directory and the path looks like run-20171023_105053-3o4933r0 where 20171023_105053 is the timestamp and 3o4933r0
is the ID of the run.</p>
<h2 id='saving-code-state'>Saving Code State</h2>
<p>When training scripts are run with <code>wandb run train.py</code> from the command line
a link is saved to the last git commit.  A diff patch is also created to the state
of the code at the time run in case there are uncommitted changes.</p>
<pre class="highlight plaintext"><code># restores the code to the state it was in when run $RUN_ID was executed
wandb restore $RUN_ID
</code></pre><h1 id='hyperparamer-search'>Hyperparamer Search</h1><h2 id='running-a-parameter-sweep'>Running a Parameter Sweep</h2>
<p>To run a parameter sweep:</p>

<ol>
<li>Initialize your project to use wandb.</li>
<li>Create a sweep.yaml file specified below, which specified your training script, your parameter
ranges and the search strategy.</li>
<li>Initialize your sweep, which gives you a SWEEP_ID and a url to track all of
your runs.</li>
<li>Run wandb agent, which will get a set of parameter names and arguments from the
wandb server and then run your training script with the parameters as arguments.  You can
run as many agents as you like.</li>
</ol>

<blockquote>
<p>Start a search from your projects root directory</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb init <span class="c"># If you haven't already initialized your project</span>
wandb sweep sweep.yaml <span class="c"># returns SWEEP_ID.</span>
</code></pre>
<blockquote>
<p>Start an agent</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb agent SWEEP_ID
</code></pre><h2 id='sweep-yaml-file'>Sweep.yaml File</h2>
<blockquote>
<p>Example sweep.yaml</p>
</blockquote>
<pre class="highlight yaml tab-yaml"><code><span class="na">description</span><span class="pi">:</span> <span class="s">random sweep for my little cnn</span>

<span class="c1"># Training script to run</span>
<span class="na">program</span><span class="pi">:</span> <span class="s">cnn.py</span>  

<span class="c1"># Method can be bayes, random, grid</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">bayes</span>

<span class="c1"># Metric to optimize</span>
<span class="na">metric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">val_loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>

<span class="c1"># Should we early terminate runs</span>
<span class="na">early_terminate</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">envelope</span>

<span class="c1"># Parameters to search over</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">learning-rate</span><span class="pi">:</span>
    <span class="na">min</span><span class="pi">:</span> <span class="s">0.1</span>
    <span class="na">max</span><span class="pi">:</span> <span class="s">0.001</span>
  <span class="na">optimizer</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">adam"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sgd"</span><span class="pi">]</span>
  <span class="na">dropout</span><span class="pi">:</span>
    <span class="na">min</span><span class="pi">:</span> <span class="s">0.01</span>
    <span class="na">max</span><span class="pi">:</span> <span class="s">0.5</span>
  <span class="na">epochs</span><span class="pi">:</span>
    <span class="na">value</span><span class="pi">:</span> <span class="s">30</span>
</code></pre><h3 id='method'>Method</h3>
<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>grid</td>
<td>Grid Search - Will iterate over all possible sets of values in parameters.</td>
</tr>
<tr>
<td>random</td>
<td>Random Search - Will choose random sets of values</td>
</tr>
<tr>
<td>bayes</td>
<td>Bayesian Optimization - Uses a gaussian process to model the function and then chooses parameters to optimize probability of improvement</td>
</tr>
</tbody></table>
<h3 id='early-terminate'>Early Terminate</h3>
<p>Early termination is a strategy to speed up hyperparameter search by killing off runs that
appear to have lower performance than successful training runs.</p>

<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>envelope</td>
<td>Use an envelope method for early termination/</td>
</tr>
<tr>
<td>Not Specified</td>
<td>Don&#39;t do early termination.</td>
</tr>
</tbody></table>
<h3 id='parameters'>Parameters</h3>
<p>The parameters dictionary specifies the ranges of configuration parameters.</p>

<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>min: (float) max: (float)</td>
<td>Continuous values between min and max</td>
</tr>
<tr>
<td>min: (int) max: (int)</td>
<td>Integers between min and max</td>
</tr>
<tr>
<td>values: [a, b, c]</td>
<td>Discrete values</td>
</tr>
<tr>
<td>value:</td>
<td>A constant</td>
</tr>
</tbody></table>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
                <a href="#" data-language-name="python--tensorflow">TensorFlow</a>
                <a href="#" data-language-name="python--keras">Keras</a>
                <a href="#" data-language-name="python--pytorch">Pytorch</a>
          </div>
      </div>
    </div>
  </body>
</html>
