
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>API Reference</title>
    <link href="images/favicon.png" rel="icon" type="image/png" />


    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .cd {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .nl {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[&quot;python--tensorflow&quot;,&quot;python--keras&quot;,&quot;python--pytorch&quot;]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo.png" class="logo" alt="Logo" />
        <div class="lang-selector">
              <a href="#" data-language-name="python--tensorflow">TensorFlow</a>
              <a href="#" data-language-name="python--keras">Keras</a>
              <a href="#" data-language-name="python--pytorch">Pytorch</a>
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc" class="toc-list-h1">
          <li>
            <a href="#introduction" class="toc-h1 toc-link" data-title="Introduction">Introduction</a>
          </li>
          <li>
            <a href="#getting-started" class="toc-h1 toc-link" data-title="Getting Started">Getting Started</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#syncing-runs-to-the-cloud" class="toc-h2 toc-link" data-title="Syncing Runs to the Cloud">Syncing Runs to the Cloud</a>
                  </li>
                  <li>
                    <a href="#automated-environments" class="toc-h2 toc-link" data-title="Automated Environments">Automated Environments</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#tracking-models" class="toc-h1 toc-link" data-title="Tracking Models">Tracking Models</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#configurations" class="toc-h2 toc-link" data-title="Configurations">Configurations</a>
                  </li>
                  <li>
                    <a href="#history" class="toc-h2 toc-link" data-title="History">History</a>
                  </li>
                  <li>
                    <a href="#media" class="toc-h2 toc-link" data-title="Media">Media</a>
                  </li>
                  <li>
                    <a href="#summary" class="toc-h2 toc-link" data-title="Summary">Summary</a>
                  </li>
                  <li>
                    <a href="#keras-callback" class="toc-h2 toc-link" data-title="Keras Callback">Keras Callback</a>
                  </li>
                  <li>
                    <a href="#saving-models" class="toc-h2 toc-link" data-title="Saving Models">Saving Models</a>
                  </li>
                  <li>
                    <a href="#restoring-code-state" class="toc-h2 toc-link" data-title="Restoring Code State">Restoring Code State</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#hyperparameter-search" class="toc-h1 toc-link" data-title="Hyperparameter Search">Hyperparameter Search</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#running-a-parameter-sweep" class="toc-h2 toc-link" data-title="Running a Parameter Sweep">Running a Parameter Sweep</a>
                  </li>
                  <li>
                    <a href="#sweep-yaml-file" class="toc-h2 toc-link" data-title="Sweep.yaml File">Sweep.yaml File</a>
                  </li>
              </ul>
          </li>
      </div>
        <ul class="toc-footer">
            <li><a href='https://wandb.ai'>Login to wandb</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id='introduction'>Introduction</h1><pre class="highlight shell tab-shell"><code><span class="c"># Install wandb</span>
pip install wandb
</code></pre>
<p>WandB is a tool that makes building machine learning models more efficient and more fun.</p>

<p>WandB helps with:</p>

<ol>
<li> Tracking, saving and reproducing models.<br></li>
<li> Visualizing results across models.</li>
<li> Automating large-scale hyperparameter search.</li>
</ol>

<p><img src="images/run-page.png" alt="Run Page" />
<img src="images/runs-tracking.png" alt="Runs Page" /></p>
<h1 id='getting-started'>Getting Started</h1>
<p>WandB is easy to install.  Once you have download the python library, you just need to add a few lines of code to your training script.</p>

<!---
WandB can be run locally or used as a cloud service.  Either way integrating wandb is pretty simple.  You can see some example learning code with integrations using Pytorch, Keras and TensorFlow at <https://github.com/wandb/examples>.

## Running Locally

> Near the top of your training script add our initialization code:

```python
# Inside my model training code
import wandb
run = wandb.init()

config = run.config 
config.dropout = 0.2
config.hidden_layer_size = 128

# Run my training code
my_train_loop()
```

```shell
# Run your script normally from the commandline
python learn.py
```

Running WandB locally:

1. Add the line `run = wandb.init()` line near the top of your training script
2. From the commandline run `wandb board` from the same directory you ran your training script. 
3. Save configuration parameters in `run.config`, save output to `run.history` and save models to `run.dir`
-->
<h2 id='syncing-runs-to-the-cloud'>Syncing Runs to the Cloud</h2><pre class="highlight shell tab-shell"><code><span class="c"># Initialize wandb in the root directory of your project</span>
wandb init
</code></pre><pre class="highlight shell tab-shell"><code><span class="c"># This runs your script and syncs all metrics and metadata to the cloud</span>
wandb run learn.py
</code></pre>
<p>Running WandB with the wandb.ai cloud service:</p>

<ol>
<li>Sign up for an account by going to our <a href="https://app.wandb.ai/login?invited">sign up page</a>.</li>
<li><p>Run <code>wandb init</code> from the command line.  You will be prompted for a team name and a project name.  This will create a
wandb directory that contains a settings file with the information you provided.  You can optionally check the <strong>wandb/settings</strong> file 
into version control.  All other files and folders in the wandb directory are automatically ignored.</p></li>
<li><p>Run your training script as <code>wandb run learn.py</code>.  Now, a new record will
be added to <em>https://app.wandb.ai/<strong>$ENTITY_NAME</strong>/<strong>$PROJECT_NAME</strong></em>.  Your training logs will be saved along with a snapshot of your latest commit.</p></li>
</ol>

<aside class="notice">
You can always rerun *wandb init* to change your project's settings.
</aside>
<h2 id='automated-environments'>Automated Environments</h2><pre class="highlight shell tab-shell"><code><span class="c"># This is secret and shouldn't be checked into version control</span>
<span class="nv">WANDB_API_KEY</span><span class="o">=</span><span class="nv">$YOUR_API_KEY</span>
<span class="c"># Description is optional</span>
<span class="nv">WANDB_DESCRIPTION</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SHORT_MESSAGE</span><span class="s2">"</span>
</code></pre><pre class="highlight shell tab-shell"><code><span class="c"># Only needed if you don't checkin the wandb/settings file</span>
<span class="nv">WANDB_ENTITY</span><span class="o">=</span><span class="nv">$username</span>
<span class="nv">WANDB_PROJECT</span><span class="o">=</span><span class="nv">$project</span>
</code></pre><pre class="highlight python tab-python"><code><span class="c"># Only needed if you don't checkin the wandb/settings file</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s">'wandb'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre><pre class="highlight python tab-python"><code><span class="c"># If you don't want to call your script with the wandb run wrapper</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WANDB_MODE'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'run'</span>
</code></pre>
<p>When you are running your script in an automated environment, you can control wandb with environment variables set before the script runs or within the script.</p>

<p>Relevant environment variables:</p>

<table><thead>
<tr>
<th>Variable name</th>
<th>Usage</th>
</tr>
</thead><tbody>
<tr>
<td><strong>WANDB_API_KEY</strong></td>
<td>Sets the authentication key associated with your account.  You can find your key at <a href="https://app.wandb.ai/profile">https://app.wandb.ai/profile</a>.  This must be set if <code>wandb login</code> hasn&#39;t been run on the remote machine.</td>
</tr>
<tr>
<td><strong>WANDB_DESCRIPTION</strong></td>
<td>Description associated with a run.  This will become the name of your run in the UI.  If not set it will be randomly generated for you</td>
</tr>
<tr>
<td><strong>WANDB_ENTITY</strong></td>
<td>The entity associated with your run.  If you have run <code>wandb init</code> in the directory of your training script, it will create a directory named <em>wandb</em> and will save a default entity which can be checked into source control.  If you don&#39;t want to create that file or want to override the file you can use the environmental variable.</td>
</tr>
<tr>
<td><strong>WANDB_PROJECT</strong></td>
<td>The project associated with your run.  This can also be set with <code>wandb init</code>, but the environmental variable will override the value.</td>
</tr>
<tr>
<td><strong>WANDB_MODE</strong></td>
<td>Set this to <em>run</em> if you want to save your run to the cloud.  Another way to do this is to run your script <code>train.py</code> with the command <code>wandb run train.py</code>.</td>
</tr>
</tbody></table>
<h1 id='tracking-models'>Tracking Models</h1><h2 id='configurations'>Configurations</h2>
<p><img src="images/configuration.png" alt="History" /></p>
<pre class="highlight python tab-python--tensorflow"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>

<span class="n">flags</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s">'data_dir'</span><span class="p">,</span> <span class="s">'/tmp/data'</span><span class="p">)</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'batch_size'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'Batch size.'</span><span class="p">)</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span><span class="p">)</span>  <span class="c"># adds all of the tensorflow flags as config variables</span>
</code></pre><pre class="highlight python tab-python--keras"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s">'N'</span><span class="p">,</span>
                     <span class="n">help</span><span class="o">=</span><span class="s">'input batch size for training (default: 8)'</span><span class="p">)</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c"># adds all of the arguments as config variables</span>
</code></pre><pre class="highlight python tab-python--pytorch"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c"># config variables are saved to the cloud</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s">'N'</span><span class="p">,</span>
                     <span class="n">help</span><span class="o">=</span><span class="s">'input batch size for training (default: 8)'</span><span class="p">)</span>
<span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c"># adds all of the arguments as config variables</span>
</code></pre>
<p>Configurations is a way of automatically tracking the hyperparameters you used
to build your model.</p>

<p>You can set the configuration values directly and access them as ordinary variables, or
you can import variables from tensorflow flags or argparse objects to integrate
with pre-existing code.</p>
<h3 id='file-based-configs'>File-Based Configs</h3>
<blockquote>
<p>(Optional) Create a config-defaults file to automatically load hyperparameters
into the config variable.</p>
</blockquote>
<pre class="highlight yaml tab-yaml"><code><span class="c1"># sample config-defaults file</span>
<span class="na">epochs</span><span class="pi">:</span>
  <span class="na">desc</span><span class="pi">:</span> <span class="s">Number of epochs to train over</span>
  <span class="na">value</span><span class="pi">:</span> <span class="s">100</span>
<span class="na">batch_size</span><span class="pi">:</span>
  <span class="na">desc</span><span class="pi">:</span> <span class="s">Size of each mini-batch</span>
  <span class="na">value</span><span class="pi">:</span> <span class="s">32</span>
</code></pre>
<p>You can create a file called config-defaults.yaml and it will automatically
be loaded into the config variable.</p>

<p>You can tell wandb to load different config files with the argument <code>--configs special-configs.yaml</code> which will load parameters from the file special-configs.yaml.</p>

<blockquote>
<p>Automatically load the yaml file into the config object</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run train.py
</code></pre>
<blockquote>
<p>Change the config file used to load the config object</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run --configs special-configs.yaml
</code></pre>
<blockquote>
<p>Mutiple config files are allowed</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb run --configs special-configs.yaml,extra-configs.yaml
</code></pre><h2 id='history'>History</h2>
<p><img src="images/history.png" alt="History" /></p>
<pre class="highlight python tab-python--tensorflow"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span><span class="p">)</span>

<span class="c"># Start training</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">run</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="c"># Run optimization op (backprop)</span>
      <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>
      <span class="c"># Calculate batch loss and accuracy</span>
      <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss_op</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>

      <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span><span class="s">'acc'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">:</span><span class="n">loss</span><span class="p">})</span>   <span class="c"># log accuracy and loss</span>
</code></pre><pre class="highlight python tab-python--keras"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">keras_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="p">):</span>
  <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span><span class="s">'loss'</span><span class="p">:</span> <span class="n">logs</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="s">'acc'</span><span class="p">:</span> <span class="n">logs</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]})</span>
  <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LambdaCallback</span><span class="p">(</span><span class="n">keras_log</span><span class="p">)])</span>
</code></pre><pre class="highlight python tab-python--pytorch"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>

  <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'model'</span><span class="p">)</span>

  <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">({</span><span class="s">"loss"</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span> <span class="s">"val_loss"</span><span class="p">:</span> <span class="n">test_loss</span><span class="p">})</span>
</code></pre>
<p>The history object is used to track metrics that change as the model trains.  You can access 
a mutable dictionary of metrics via <code>run.history.row</code>.  The row will be saved and a new row created when 
<code>run.history.add</code> is called.  For simplicity, you can call run.history.add and pass in a dictionary of all the metrics you would like to save.</p>
<h3 id='context-manager'>Context Manager</h3>
<p>We provide a context manager that automatically calls <code>add</code>
and accepts an optional boolean to help keep nested code clean.</p>

<blockquote>
<p>Context manager</p>
</blockquote>
<pre class="highlight python tab-python"><code><span class="k">with</span> <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">batch_idx</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
  <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">row</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">"metric"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
  <span class="k">if</span> <span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">compute</span><span class="p">:</span>
    <span class="c"># Something expensive here</span>
</code></pre><h2 id='media'>Media</h2><pre class="highlight python tab-python"><code><span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">row</span><span class="p">[</span><span class="s">"examples"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">numpy_array_or_pil</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="s">"Label"</span><span class="p">)]</span>
<span class="n">run</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">add</span><span class="p">()</span>
</code></pre>
<p>The history object also accepts rich media.  Currently only images are supported.  Media is added
by supplying a list of wandb media objects.</p>

<p>If a numpy array is supplied we assume it&#39;s gray scale if the last dimension is 1, RGB if it&#39;s 3, 
and RGBA if it&#39;s 4.  If the array contains floats we convert them to ints between 0 and 255.<br>
You can specify a <a href="https://pillow.readthedocs.io/en/3.1.x/handbook/concepts.html#concept-modes">mode</a> 
manually or just supply a <code>PIL.Image</code>.  We recommend you don&#39;t add more than 20-50 images per step.</p>
<h2 id='summary'>Summary</h2>
<p><img src="images/summary.png" alt="Summary" /></p>
<pre class="highlight python tab-python"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>
  <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_accuracy</span>
</code></pre>
<p>The summary statistics are used to track single metrics per model.  If a summary
metric is modified, only the updated state is saved.  We automatically set summary to the last
history row added unless you modify it manually.</p>
<h2 id='keras-callback'>Keras Callback</h2>
<blockquote>
<p>Simpler way to track metrics in keras using wandb.callbacks.Keras</p>
</blockquote>
<pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Keras</span><span class="p">()])</span>
</code></pre>
<p>If you are using keras, you can use the Keras callback to automatically save
all the metrics and the loss values tracked in <code>model.fit</code>.</p>
<h2 id='saving-models'>Saving Models</h2>
<p><img src="images/saved.png" alt="Saved" /></p>
<pre class="highlight python tab-python--keras"><code><span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Keras</span><span class="p">()])</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run</span><span class="o">.</span><span class="nb">dir</span><span class="p">,</span> <span class="s">"model.h5"</span><span class="p">))</span> <span class="c">#</span>
</code></pre>
<p>Wandb will save to the cloud any files put in wandb&#39;s run directory.</p>

<p>Wandb&#39;s run directories are inside the wandb directory and the path looks like <em>run-20171023_105053-3o4933r0</em> where <em>20171023_105053</em> is the timestamp and <em>3o4933r0</em> is the ID of the run.</p>
<h2 id='restoring-code-state'>Restoring Code State</h2><pre class="highlight plaintext"><code># creates a branch and restores the code to the state it was in when run $RUN_ID was executed
wandb restore $RUN_ID
</code></pre>
<p>When <code>wandb.init</code> is called from your script, a link is saved to the last git commit if the code
is in a git repository.  A diff patch is also created in case there are uncommitted changes or changes
that are out of sync with your remote.</p>
<h1 id='hyperparameter-search'>Hyperparameter Search</h1><h2 id='running-a-parameter-sweep'>Running a Parameter Sweep</h2>
<p>To run a parameter sweep:</p>

<ol>
<li>Initialize your project to use wandb in the cloud.</li>
<li>Create a sweep.yaml file specified below, which specified your training script, your parameter
ranges and the search strategy.</li>
<li>Initialize your sweep, which gives you a SWEEP_ID and a url to track all of
your runs.</li>
<li>Run wandb agent, which will get a set of parameter names and arguments from the
wandb server and then run your training script with the parameters as arguments.  You can
run as many agents as you like.</li>
</ol>

<blockquote>
<p>Start a search from your projects root directory</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb init <span class="c"># If you haven't already initialized your project</span>
wandb sweep sweep.yaml <span class="c"># returns SWEEP_ID.</span>
</code></pre>
<blockquote>
<p>Start an agent</p>
</blockquote>
<pre class="highlight shell tab-shell"><code>wandb agent SWEEP_ID
</code></pre><h2 id='sweep-yaml-file'>Sweep.yaml File</h2>
<blockquote>
<p>Example sweep.yaml</p>
</blockquote>
<pre class="highlight yaml tab-yaml"><code><span class="na">description</span><span class="pi">:</span> <span class="s">random sweep for my little cnn</span>

<span class="c1"># Training script to run</span>
<span class="na">program</span><span class="pi">:</span> <span class="s">cnn.py</span>  

<span class="c1"># Method can be bayes, random, grid</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">bayes</span>

<span class="c1"># Metric to optimize</span>
<span class="na">metric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">val_loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>

<span class="c1"># Should we early terminate runs</span>
<span class="na">early_terminate</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">envelope</span>

<span class="c1"># Parameters to search over</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">learning-rate</span><span class="pi">:</span>
    <span class="na">min</span><span class="pi">:</span> <span class="s">0.001</span>
    <span class="na">max</span><span class="pi">:</span> <span class="s">0.1</span>
  <span class="na">optimizer</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">adam"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sgd"</span><span class="pi">]</span>
  <span class="na">dropout</span><span class="pi">:</span>
    <span class="na">min</span><span class="pi">:</span> <span class="s">0.01</span>
    <span class="na">max</span><span class="pi">:</span> <span class="s">0.5</span>
  <span class="na">epochs</span><span class="pi">:</span>
    <span class="na">value</span><span class="pi">:</span> <span class="s">30</span>
</code></pre><h3 id='method'>Method</h3>
<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>grid</td>
<td>Grid Search - Will iterate over all possible sets of values in parameters.</td>
</tr>
<tr>
<td>random</td>
<td>Random Search - Will choose random sets of values</td>
</tr>
<tr>
<td>bayes</td>
<td>Bayesian Optimization - Uses a gaussian process to model the function and then chooses parameters to optimize probability of improvement</td>
</tr>
</tbody></table>
<h3 id='early-terminate'>Early Terminate</h3>
<p>Early termination is a strategy to speed up hyperparameter search by killing off runs that
appear to have lower performance than successful training runs.</p>

<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>envelope</td>
<td>Use an envelope method for early termination/</td>
</tr>
<tr>
<td>Not Specified</td>
<td>Don&#39;t do early termination.</td>
</tr>
</tbody></table>
<h3 id='parameters'>Parameters</h3>
<p>The parameters dictionary specifies the ranges of configuration parameters.</p>

<table><thead>
<tr>
<th>Values</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>distribution:</td>
<td>A distribution from the distrbution table below.  If not specified, the sweep will set to uniform is max and min are set, categorical if values are set and constant if value is set.</td>
</tr>
<tr>
<td>min: (float) max: (float)</td>
<td>Continuous values between min and max</td>
</tr>
<tr>
<td>min: (int) max: (int)</td>
<td>Integers between min and max</td>
</tr>
<tr>
<td>values: [a, b, c]</td>
<td>Discrete values</td>
</tr>
<tr>
<td>value:</td>
<td>A constant</td>
</tr>
<tr>
<td>mu:</td>
<td>Mean for normal or lognormal distributions</td>
</tr>
<tr>
<td>sigma:</td>
<td>Std Dev for normal or lognormal distributions</td>
</tr>
<tr>
<td>q:</td>
<td>Quantization parameter for quantized distributions</td>
</tr>
</tbody></table>
<h3 id='distributions'>Distributions</h3>
<p>Supported distributions</p>

<table><thead>
<tr>
<th>Name</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>constant</td>
<td>Constant distribution.  Must specify value.</td>
</tr>
<tr>
<td>categorical</td>
<td>Categorical distribution.  Must specify values.</td>
</tr>
<tr>
<td>int_uniform</td>
<td>Uniform integer.  Must specify max and min as integers.</td>
</tr>
<tr>
<td>uniform</td>
<td>Uniform continuous.  Must specify max and min as floats.</td>
</tr>
<tr>
<td>q_uniform</td>
<td>Quantized uniform.  Returns  round(X / q) * q where X is uniform.  Q defaults to 1.</td>
</tr>
<tr>
<td>log_uniform</td>
<td>Log uniform.  Number between exp(min) and exp(max) so that the logarithm of the return value is uniformly distributed.</td>
</tr>
<tr>
<td>q_log_uniform</td>
<td>Quantized log uniform.  Returns  round(X / q) * q where X is log_uniform.  Q defaults to 1.</td>
</tr>
<tr>
<td>normal</td>
<td>Normal distribution.  Value is chosen from normal distribution.  Can set mean mu (default 0) and std dev sigma (default 1).</td>
</tr>
<tr>
<td>q_normal</td>
<td>Quantized normal distribution.  Returns  round(X / q) * q where X is normal.  Q defaults to 1.</td>
</tr>
<tr>
<td>log_normal</td>
<td>Log normal distribution. Value is chosen from log normal distribution.  Can set mean mu (default 0) and std dev sigma (default 1).</td>
</tr>
<tr>
<td>q_log_normal</td>
<td>Quantized log normal distribution.  Returns  round(X / q) * q where X is log_normal.  Q defaults to 1.</td>
</tr>
</tbody></table>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
                <a href="#" data-language-name="python--tensorflow">TensorFlow</a>
                <a href="#" data-language-name="python--keras">Keras</a>
                <a href="#" data-language-name="python--pytorch">Pytorch</a>
          </div>
      </div>
    </div>
  </body>
</html>
